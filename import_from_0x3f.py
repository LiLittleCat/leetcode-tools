"""
ä» LeetCode è®¨è®ºé¡µé¢æ‹‰å–é¢˜å•æ•°æ®å¹¶åˆ›å»º LeetCode é¢˜å•

æ•°æ®æ¥æº: https://leetcode.cn/circle/discuss/
"""

import os
import requests
import json
import re
import argparse
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from collections import defaultdict
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from leetcode_favorite import LeetCodeClient


LEETCODE_DISCUSS_PRE_URL = "https://leetcode.cn/circle/discuss/"

# æœ¬åœ°ä¿å­˜ç›®å½•
LOCAL_HTML_DIR = os.path.join(os.path.dirname(__file__), "discuss_html")

DISCUSSION_URL_MAP = {
    "0viNMK": {
        "filename": "sliding_window",
        "title": "æ»‘åŠ¨çª—å£ä¸åŒæŒ‡é’ˆ"
    },
    "SqopEo": {
        "filename": "binary_search",
        "title": "äºŒåˆ†æŸ¥æ‰¾"
    },
    "9oZFK9": {
        "filename": "monotonic_stack",
        "title": "å•è°ƒæ ˆ"
    },
    "YiXPXW": {
        "filename": "grid",
        "title": "ç½‘æ ¼å›¾"
    },
    "dHn9Vk": {
        "filename": "bitwise_operations",
        "title": "ä½è¿ç®—"
    },
    "01LUak": {
        "filename": "graph",
        "title": "å›¾è®º"
    },
    "tXLS3i": {
        "filename": "dynamic_programming",
        "title": "DP"
    },
    "mOr1u6": {
        "filename": "data_structure",
        "title": "æ•°æ®ç»“æ„"
    },
    "IYT3ss": {
        "filename": "math",
        "title": "æ•°å­¦ç®—æ³•"
    },
    "g6KTKL": {
        "filename": "greedy",
        "title": "è´ªå¿ƒä¸æ€ç»´"
    },
    "K0n2gO": {
        "filename": "trees",
        "title": "é“¾è¡¨ã€æ ‘ä¸å›æº¯"
    },
    "SJFwQI": {
        "filename": "string",
        "title": "å­—ç¬¦ä¸²"
    },
}

# åˆ†ç±»åˆ—è¡¨ï¼š(discuss_id, filename, title)
PROBLEM_CATEGORIES = [
    (discuss_id, info["filename"], info["title"]) 
    for discuss_id, info in DISCUSSION_URL_MAP.items()
]


@dataclass
class ProblemInfo:
    """é¢˜ç›®ä¿¡æ¯"""
    title: str
    slug: str
    is_premium: bool = False


def fetch_discussion_html(discuss_id: str) -> Optional[str]:
    """
    ä» LeetCode è·å–è®¨è®ºé¡µé¢çš„ HTML
    :param discuss_id: è®¨è®º IDï¼Œå¦‚ "0viNMK"
    :return: HTML å†…å®¹
    """
    url = f"{LEETCODE_DISCUSS_PRE_URL}{discuss_id}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    
    try:
        print(f"æ­£åœ¨è·å–: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"è·å–è®¨è®ºé¡µé¢å¤±è´¥: {e}")
        return None


def extract_heading_and_list_elements(html_content: str) -> str:
    """
    ä» HTML ä¸­æå– h1, h2, h3, ul, li å…ƒç´ 
    :param html_content: åŸå§‹ HTML å†…å®¹
    :return: æå–åçš„ç²¾ç®€ HTML
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # åˆ›å»ºæ–°çš„ HTML æ–‡æ¡£
    new_soup = BeautifulSoup("<html><head><meta charset='utf-8'></head><body></body></html>", 'html.parser')
    body = new_soup.find('body')
    
    # æŸ¥æ‰¾æ–‡ç« å†…å®¹åŒºåŸŸ
    # ä¼˜å…ˆæŸ¥æ‰¾ 'break-words' (å¸¸è§äºåŠ¨æ€æ¸²æŸ“çš„ LeetCode è®¨è®ºé¡µ)
    content_area = soup.find('div', class_=re.compile(r'break-words', re.I))
    if not content_area:
        content_area = soup.find('div', class_=re.compile(r'content|article|post|topic', re.I))
    if not content_area:
        content_area = soup
    
    # æå–æ‰€æœ‰ h1, h2, h3, h4, ul, ol, li å…ƒç´ 
    allowed_tags = ['h1', 'h2', 'h3', 'h4', 'ul', 'ol', 'li', 'a', 'p']
    
    def clone_element(element, parent):
        """é€’å½’å…‹éš†å…ƒç´ ï¼Œåªä¿ç•™å…è®¸çš„æ ‡ç­¾"""
        if element.name in allowed_tags:
            # è¿‡æ»¤æ‰ä¸åŒ…å«é¢˜ç›®é“¾æ¥çš„ li å…ƒç´ 
            if element.name == 'li':
                has_problem_link = False
                for a_tag in element.find_all('a'):
                    href = a_tag.get('href', '')
                    if href and 'problems' in href:
                        has_problem_link = True
                        break
                if not has_problem_link:
                    return

            new_tag = new_soup.new_tag(element.name)
            # ä¿ç•™ href å±æ€§
            if element.name == 'a' and element.get('href'):
                new_tag['href'] = element.get('href')
            
            for child in element.children:
                if hasattr(child, 'name') and child.name:
                    clone_element(child, new_tag)
                elif child.string:
                    new_tag.append(child.string.strip())
            
            if new_tag.get_text(strip=True):  # åªæ·»åŠ æœ‰å†…å®¹çš„å…ƒç´ 
                parent.append(new_tag)
    
    # æŸ¥æ‰¾æ‰€æœ‰æ ‡é¢˜å’Œåˆ—è¡¨
    for tag in content_area.find_all(['h1', 'h2', 'h3', 'h4', 'ul', 'ol']):
        clone_element(tag, body)
    
    # ä½¿ç”¨ç´§å‡‘æ ¼å¼é¿å…å†™å…¥æ—¶è‡ªåŠ¨æ¢è¡Œ/ç¼©è¿›
    return new_soup.decode(formatter="minimal")


def fetch_and_save_discussion_html(discuss_id: str, filename: str) -> bool:
    """
    è·å–è®¨è®ºé¡µé¢ HTML å¹¶ä¿å­˜åˆ°æœ¬åœ°
    :param discuss_id: è®¨è®º ID
    :param filename: ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    :return: æ˜¯å¦æˆåŠŸ
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(LOCAL_HTML_DIR, exist_ok=True)
    
    # è·å– HTML
    html_content = fetch_discussion_html(discuss_id)
    if not html_content:
        return False
    
    # æå–ç²¾ç®€å†…å®¹
    simplified_html = extract_heading_and_list_elements(html_content)
    
    # ä¿å­˜ç²¾ç®€ HTML
    filepath = os.path.join(LOCAL_HTML_DIR, f"{filename}.html")
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(simplified_html)
    
    print(f"ç²¾ç®€ HTML å·²ä¿å­˜åˆ°: {filepath}")
    return True


def fetch_all_discussions() -> None:
    """
    è·å–æ‰€æœ‰è®¨è®ºé¡µé¢å¹¶ä¿å­˜
    """
    print(f"\nå°†è·å– {len(DISCUSSION_URL_MAP)} ä¸ªè®¨è®ºé¡µé¢...")
    
    success_count = 0
    for discuss_id, info in DISCUSSION_URL_MAP.items():
        if fetch_and_save_discussion_html(discuss_id, info["filename"]):
            success_count += 1
    
    print(f"\nå®Œæˆ: æˆåŠŸ {success_count}/{len(DISCUSSION_URL_MAP)} ä¸ª")


def parse_section_title(title: str) -> Tuple[str, str]:
    """
    è§£ææ ‡é¢˜ï¼Œæå–åºå·å’Œåç§°
    :param title: åŸå§‹æ ‡é¢˜ï¼Œå¦‚ "ä¸€ã€å®šé•¿...", "Â§1.1 åŸºç¡€"
    :return: (åºå·, åç§°)ï¼Œå³ ("1", "å®šé•¿...") æˆ– ("1.1", "åŸºç¡€")
    """
    if not title:
        return "", ""
    
    # æ¸…ç† zero-width spaces ç­‰ä¸å¯è§å­—ç¬¦
    title = title.strip()
        
    # 1. å¤„ç† Â§ æ ¼å¼ (Â§1.1 åŸºç¡€)
    match = re.match(r'^Â§([\d.]+)\s*(.*)', title)
    if match:
        return match.group(1), match.group(2)
        
    # 2. å¤„ç†ä¸­æ–‡æ•°å­—æ ¼å¼ (ä¸€ã€å®šé•¿...)
    cn_nums = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"
    match = re.match(rf'^([{cn_nums}]+)ã€\s*(.*)', title)
    if match:
        cn_num = match.group(1)
        name = match.group(2)
        
        # ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—
        val = 0
        if cn_num == 'å':
            val = 10
        elif cn_num.startswith('å'):
            # åä¸€, åäºŒ...
            val = 10 + ("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å".index(cn_num[1]) + 1)
        elif cn_num.endswith('å') and len(cn_num) == 2:
             # äºŒå, ä¸‰å...
            val = ("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å".index(cn_num[0]) + 1) * 10
        elif len(cn_num) == 1:
            val = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å".index(cn_num) + 1
            
        if val > 0:
            return str(val), name
        
    return "", title


def compact_name_parts(name_parts: List[str], max_length: int = 30, min_part_len: int = 4) -> str:
    """
    æ‹¼æ¥åç§°ï¼Œå¹¶åœ¨è¶…é•¿æ—¶æŒ‰å„éƒ¨åˆ†å‡åŒ€ç¼©å‡
    :param name_parts: ç»„æˆåç§°çš„å„æ®µ
    :param max_length: å…è®¸çš„æœ€å¤§æ€»é•¿åº¦
    :param min_part_len: æ¯æ®µçš„æœ€å°ä¿ç•™é•¿åº¦
    :return: ç¼©å‡åçš„åç§°
    """
    parts = [p.strip() for p in name_parts if p and p.strip()]
    if not parts:
        return "æœªåˆ†ç±»"

    total_len = sum(len(p) for p in parts) + (len(parts) - 1)
    if total_len <= max_length:
        return "-".join(parts)

    parts = parts[:]  # copy before mutation
    # è½®è¯¢å¼ç¼©å‡ï¼Œæ¯æ®µå°½é‡å°‘ç ä¸€ç‚¹ï¼Œä¿æŒå¯è¯»æ€§
    while True:
        total_len = sum(len(p) for p in parts) + (len(parts) - 1)
        if total_len <= max_length:
            break

        reduced = False
        for i, p in enumerate(parts):
            if len(p) > min_part_len and total_len > max_length:
                parts[i] = p[:-1]
                total_len -= 1
                reduced = True

        if not reduced:  # æ‰€æœ‰æ®µéƒ½åˆ°è¾¾æœ€å°é•¿åº¦ï¼Œæœ€ååšç¡¬æˆªæ–­å…œåº•
            joined = "-".join(parts)
            return joined[:max_length]

    return "-".join(parts)


def extract_slug_from_href(href: str) -> Optional[str]:
    """
    ä»é“¾æ¥ä¸­æå–é¢˜ç›® slug
    :param href: é¢˜ç›®é“¾æ¥
    :return: é¢˜ç›® slug
    """
    if not href or 'problems' not in href:
        return None
    
    # åŒ¹é… /problems/xxx/ æˆ– /problems/xxx
    match = re.search(r'/problems/([^/?#]+)', href)
    if match:
        return match.group(1)
    return None


def parse_html_to_categories(html_filepath: str, root_title: str, category_index: int) -> List[Tuple[str, List[ProblemInfo]]]:
    """
    è§£æ HTML æ–‡ä»¶ï¼Œæå–åˆ†ç±»å’Œé¢˜ç›®ä¿¡æ¯
    :param html_filepath: HTML æ–‡ä»¶è·¯å¾„
    :param root_title: æ ¹åˆ†ç±»æ ‡é¢˜ï¼ˆå¦‚ "æ»‘åŠ¨çª—å£ä¸åŒæŒ‡é’ˆ"ï¼‰
    :param category_index: åˆ†ç±»åœ¨åˆ—è¡¨ä¸­çš„åºå·ï¼ˆä¸€çº§åºå·ï¼‰
    :return: [(åˆ†ç±»åç§°, é¢˜ç›®åˆ—è¡¨), ...]
    """
    if not os.path.exists(html_filepath):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {html_filepath}")
        return []
    
    with open(html_filepath, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    body = soup.find('body')
    if not body:
        return []
    
    results = []
    h2_seq = 0
    h3_seq_map: Dict[int, int] = defaultdict(int)  # per h2
    h4_seq_map: Dict[Tuple[int, int], int] = defaultdict(int)  # per (h2, h3)
    
    # éå†æ‰€æœ‰å…ƒç´ ï¼Œæ„å»ºå±‚çº§ç»“æ„
    current_h2 = ""  # å½“å‰ h2 æ ‡é¢˜ï¼ˆå¦‚ "ä¸€ã€å®šé•¿æ»‘åŠ¨çª—å£"ï¼‰
    current_h2_idx = 0
    current_h2_name = ""
    current_h3 = ""  # å½“å‰ h3 æ ‡é¢˜ï¼ˆå¦‚ "Â§1.1 åŸºç¡€"ï¼‰
    current_h3_idx = 0
    current_h3_name = ""
    current_h4 = ""  # å½“å‰ h4 æ ‡é¢˜
    current_h4_idx = 0
    current_h4_name = ""
    
    for element in body.children:
        if not hasattr(element, 'name') or not element.name:
            continue
        
        if element.name == 'h2':
            current_h2 = element.get_text(strip=True)
            h2_seq += 1
            _, current_h2_name = parse_section_title(current_h2)
            current_h2_idx = h2_seq  # ä¸€çº§å†…çš„äºŒçº§åºå·ä½¿ç”¨é¡ºåº
            current_h3 = ""  # é‡ç½® h3
            current_h3_idx = 0
            current_h3_name = ""
            current_h4 = ""
            current_h4_idx = 0
            current_h4_name = ""
            h3_seq_map[current_h2_idx] = 0
            h4_seq_map[(current_h2_idx, 0)] = 0
            
        elif element.name == 'h3':
            current_h3 = element.get_text(strip=True)
            _, current_h3_name = parse_section_title(current_h3)
            h3_seq_map[current_h2_idx] += 1
            current_h3_idx = h3_seq_map[current_h2_idx]
            current_h4 = ""
            current_h4_idx = 0
            current_h4_name = ""
            h4_seq_map[(current_h2_idx, current_h3_idx)] = 0

        elif element.name == 'h4':
            current_h4 = element.get_text(strip=True)
            _, current_h4_name = parse_section_title(current_h4)
            key = (current_h2_idx, current_h3_idx)
            h4_seq_map[key] += 1
            current_h4_idx = h4_seq_map[key]
            
        elif element.name == 'ul':
            # æ”¶é›†è¿™ä¸ª ul ä¸­çš„æ‰€æœ‰é¢˜ç›®
            problems = []
            for li in element.find_all('li', recursive=False):
                a_tag = li.find('a')
                if a_tag:
                    href = a_tag.get('href', '')
                    slug = extract_slug_from_href(href)
                    if slug:
                        title = a_tag.get_text(strip=True)
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜é¢˜
                        li_text = li.get_text()
                        is_premium = 'ä¼šå‘˜é¢˜' in li_text or 'ğŸ”’' in li_text
                        problems.append(ProblemInfo(
                            title=title,
                            slug=slug,
                            is_premium=is_premium
                        ))
            
            if problems:
                h2_idx = current_h2_idx
                h2_name = current_h2_name
                h3_idx = current_h3_idx
                h3_name = current_h3_name
                h4_idx = current_h4_idx
                h4_name = current_h4_name
                
                number_parts = [str(category_index)] if category_index else []
                if h2_idx:
                    number_parts.append(str(h2_idx))
                if h3_idx:
                    number_parts.append(str(h3_idx))
                if h4_idx:
                    number_parts.append(str(h4_idx))
                number_str = ".".join(number_parts) if number_parts else ""
                
                name_parts = [number_str] if number_str else []
                h2_display = None
                
                if h4_idx or h4_name:
                    # æœ‰ h4ï¼š åºå·-h2-h3-h4ï¼ˆè‹¥æ—  h3 åˆ™è·³è¿‡ h3ï¼‰
                    h2_display = h2_name or current_h2
                    h3_display = h3_name or current_h3
                    h4_display = h4_name or current_h4
                    if h2_display:
                        name_parts.append(h2_display)
                    if h3_display:
                        name_parts.append(h3_display)
                    if h4_display:
                        name_parts.append(h4_display)
                elif h3_idx or h3_name:
                    # æœ‰ h3ï¼š åºå·-h2-h3
                    h2_display = h2_name or current_h2
                    h3_display = h3_name or current_h3
                    if h2_display:
                        name_parts.append(h2_display)
                    if h3_display:
                        name_parts.append(h3_display)
                else:
                    # æ—  h3ï¼šåºå·-åˆ†ç±»-h2
                    if root_title:
                        name_parts.append(root_title)
                    h2_display = h2_name or current_h2
                    if h2_display:
                        name_parts.append(h2_display)
                
                # æ‹¼æ¥åç§°ï¼ˆè¶…é•¿æ—¶æŒ‰å„æ®µå‡åŒ€ç¼©å‡ï¼‰
                full_name = compact_name_parts(name_parts, max_length=30)
                # å¦‚æœä»ç„¶è¶…é•¿ï¼Œä¼˜å…ˆå»æ‰ h2 ä»¥ä¿ç•™æ›´æ·±å±‚çš„æ ‡é¢˜
                if len(full_name) > 30 and h2_display:
                    name_parts_no_h2 = [p for p in name_parts if p != h2_display]
                    if name_parts_no_h2:
                        full_name = compact_name_parts(name_parts_no_h2, max_length=30)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒåç§°çš„åˆ†ç±»ï¼Œå¦‚æœæœ‰åˆ™åˆå¹¶
                existing = None
                for i, (name, probs) in enumerate(results):
                    if name == full_name:
                        existing = i
                        break
                
                if existing is not None:
                    # åˆå¹¶é¢˜ç›®
                    existing_slugs = {p.slug for p in results[existing][1]}
                    for p in problems:
                        if p.slug not in existing_slugs:
                            results[existing][1].append(p)
                else:
                    results.append((full_name, problems))
    
    return results


def load_category_from_html(filename: str, title: str, category_index: int) -> List[Tuple[str, List[ProblemInfo]]]:
    """
    ä»æœ¬åœ° HTML æ–‡ä»¶åŠ è½½åˆ†ç±»ä¿¡æ¯
    :param filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    :param title: åˆ†ç±»æ ‡é¢˜
    :param category_index: åˆ†ç±»åºå·
    :return: [(åˆ†ç±»åç§°, é¢˜ç›®åˆ—è¡¨), ...]
    """
    filepath = os.path.join(LOCAL_HTML_DIR, f"{filename}.html")
    return parse_html_to_categories(filepath, title, category_index)


def create_favorite_from_category(
    client: LeetCodeClient,
    category_name: str,
    problems: List[ProblemInfo],
    dry_run: bool = False
) -> Optional[str]:
    """
    ä»åˆ†ç±»åˆ›å»ºé¢˜å•
    :param client: LeetCode å®¢æˆ·ç«¯
    :param category_name: åˆ†ç±»åç§°
    :param problems: é¢˜ç›®åˆ—è¡¨
    :param dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œ
    :return: é¢˜å• slug
    """
    # è¿‡æ»¤æ‰ä¼šå‘˜é¢˜
    problems = [p for p in problems if not p.is_premium]
    
    if not problems:
        print(f"åˆ†ç±» [{category_name}] æ²¡æœ‰éä¼šå‘˜é¢˜ç›®ï¼Œè·³è¿‡")
        return None
    
    # æ„å»ºé¢˜å•åç§°
    favorite_name = category_name
    
    # å†æ¬¡ç¡®ä¿ä¸è¶…è¿‡30å­—ç¬¦
    if len(favorite_name) > 30:
        favorite_name = favorite_name[:27] + "..."
    
    if dry_run:
        print(f"[è¯•è¿è¡Œ] å°†åˆ›å»ºé¢˜å•: {favorite_name}")
        print(f"  åŒ…å« {len(problems)} é“é¢˜ç›®:")
        for i, p in enumerate(problems[:5], 1):
            print(f"    {i}. {p.title} ({p.slug})")
        if len(problems) > 5:
            print(f"    ... è¿˜æœ‰ {len(problems) - 5} é“é¢˜ç›®")
        return None
    
    # å®é™…åˆ›å»ºé¢˜å•
    print(f"æ­£åœ¨åˆ›å»ºé¢˜å•: {favorite_name}")
    
    favorite_slug = client.create_favorite_list(favorite_name, is_public=False, description=f"é¢˜å•: {category_name}")
    
    if not favorite_slug:
        print(f"åˆ›å»ºé¢˜å•å¤±è´¥: {favorite_name}")
        return None
    
    print(f"é¢˜å•åˆ›å»ºæˆåŠŸ: {favorite_name} (slug: {favorite_slug})")
    
    # è·å–é¢˜ç›® slugs
    slugs = [p.slug for p in problems]
    
    # åˆ†æ‰¹æ·»åŠ ï¼Œæ¯æ‰¹æœ€å¤š 50 ä¸ª
    batch_size = 50
    total_added = 0
    
    for i in range(0, len(slugs), batch_size):
        batch = slugs[i:i + batch_size]
        if client.batch_add_questions_to_favorite(favorite_slug, batch):
            total_added += len(batch)
            print(f"  å·²æ·»åŠ  {total_added}/{len(slugs)} é“é¢˜ç›®")
        else:
            print(f"  æ‰¹é‡æ·»åŠ å¤±è´¥ï¼Œå½“å‰ä½ç½®: {i}")
    
    print(f"å®Œæˆ: å…±æ·»åŠ  {total_added} é“é¢˜ç›®åˆ°é¢˜å• [{favorite_name}]")
    return favorite_slug


def display_available_categories():
    """æ˜¾ç¤ºå¯ç”¨çš„åˆ†ç±»åˆ—è¡¨"""
    print("\nå¯ç”¨çš„é¢˜å•åˆ†ç±»:")
    print("-" * 50)
    for i, (discuss_id, filename, title) in enumerate(PROBLEM_CATEGORIES, 1):
        print(f"{i:2}. {title} ({filename})")
    print("-" * 50)



def interactive_mode(client: LeetCodeClient):
    """
    äº¤äº’æ¨¡å¼
    :param client: LeetCode å®¢æˆ·ç«¯
    """
    while True:
        display_available_categories()
        print("\næ“ä½œé€‰é¡¹:")
        print("1. è·å–è®¨è®ºé¡µé¢ HTMLï¼ˆå•ä¸ªï¼‰")
        print("2. è·å–æ‰€æœ‰è®¨è®ºé¡µé¢ HTML")
        print("3. åˆ›å»ºæŒ‡å®šåˆ†ç±»çš„å­é¢˜å•")
        print("4. åˆ›å»ºæ‰€æœ‰åˆ†ç±»çš„å­é¢˜å•")
        print("q. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ: ").strip().lower()
        
        if choice == 'q':
            break
        
        if choice == '1':
            # è·å–å•ä¸ªè®¨è®ºé¡µé¢ HTML
            cat_input = input("\nè¯·è¾“å…¥åˆ†ç±»ç¼–å· (1-12): ").strip()
            try:
                cat_index = int(cat_input) - 1
                if 0 <= cat_index < len(PROBLEM_CATEGORIES):
                    discuss_id, filename, title = PROBLEM_CATEGORIES[cat_index]
                    fetch_and_save_discussion_html(discuss_id, filename)
                else:
                    print("æ— æ•ˆçš„åˆ†ç±»ç¼–å·")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                
        elif choice == '2':
            # è·å–æ‰€æœ‰è®¨è®ºé¡µé¢ HTML
            fetch_all_discussions()
            
        elif choice == '3':
            # åˆ›å»ºæŒ‡å®šåˆ†ç±»çš„å­é¢˜å•
            cat_input = input("\nè¯·è¾“å…¥åˆ†ç±»ç¼–å· (1-12): ").strip()
            try:
                cat_index = int(cat_input) - 1
                if 0 <= cat_index < len(PROBLEM_CATEGORIES):
                    discuss_id, filename, title = PROBLEM_CATEGORIES[cat_index]
                    
                    # ä» HTML æ–‡ä»¶åŠ è½½åˆ†ç±»
                    categories = load_category_from_html(filename, title, cat_index + 1)
                    
                    if not categories:
                        print(f"æœªæ‰¾åˆ°åˆ†ç±»æ•°æ®ï¼Œè¯·å…ˆä½¿ç”¨é€‰é¡¹ 1 è·å– HTML")
                        continue
                    
                    print(f"\næ‰¾åˆ° {len(categories)} ä¸ªå­åˆ†ç±»:")
                    total_problems = 0
                    for i, (name, problems) in enumerate(categories, 1):
                        non_premium = [p for p in problems if not p.is_premium]
                        total_problems += len(non_premium)
                        print(f"{i:3}. {name}")
                    
                    confirm = input(f"\nå°†åˆ›å»º {len(categories)} ä¸ªé¢˜å•ï¼ˆå…± {total_problems} é“é¢˜ï¼‰ï¼Œç¡®è®¤ï¼Ÿ(y/n): ").strip().lower()
                    if confirm == 'y':
                        for name, problems in categories:
                            create_favorite_from_category(client, name, problems)
                else:
                    print("æ— æ•ˆçš„åˆ†ç±»ç¼–å·")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                
        elif choice == '4':
            # åˆ›å»ºæ‰€æœ‰åˆ†ç±»çš„å­é¢˜å•
            print("\nç»Ÿè®¡æ‰€æœ‰åˆ†ç±»çš„å­é¢˜å•...")
            
            all_categories = []
            for idx, (discuss_id, filename, title) in enumerate(PROBLEM_CATEGORIES):
                categories = load_category_from_html(filename, title, idx + 1)
                all_categories.extend(categories)
            
            if not all_categories:
                print("æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»æ•°æ®ï¼Œè¯·å…ˆä½¿ç”¨é€‰é¡¹ 2 è·å–æ‰€æœ‰ HTML")
                continue
            
            total_problems = sum(len([p for p in probs if not p.is_premium]) for _, probs in all_categories)
            print(f"\næ‰¾åˆ° {len(all_categories)} ä¸ªå­åˆ†ç±»ï¼Œå…± {total_problems} é“é¢˜")
            
            confirm = input(f"\nå°†åˆ›å»º {len(all_categories)} ä¸ªé¢˜å•ï¼Œç¡®è®¤ï¼Ÿ(y/n): ").strip().lower()
            if confirm == 'y':
                for name, problems in all_categories:
                    create_favorite_from_category(client, name, problems)
                    
        else:
            print("æ— æ•ˆçš„é€‰é¡¹")


def main():
    parser = argparse.ArgumentParser(description='ä» LeetCode è®¨è®ºé¡µé¢å¯¼å…¥é¢˜å•æ•°æ®')
    parser.add_argument('--fetch-all', action='store_true', help='è·å–æ‰€æœ‰è®¨è®ºé¡µé¢ HTML')
    parser.add_argument('--fetch', type=int, help='è·å–æŒ‡å®šåˆ†ç±»çš„è®¨è®ºé¡µé¢ HTML (1-12)')
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    load_dotenv(env_path)
    
    csrf_token = os.getenv('csrftoken')
    session_id = os.getenv('LEETCODE_SESSION')
    
    if args.fetch_all:
        fetch_all_discussions()
    elif args.fetch:
        if 1 <= args.fetch <= len(PROBLEM_CATEGORIES):
            discuss_id, filename, title = PROBLEM_CATEGORIES[args.fetch - 1]
            fetch_and_save_discussion_html(discuss_id, filename)
        else:
            print(f"æ— æ•ˆçš„åˆ†ç±»ç¼–å·: {args.fetch}")
    else:
        if not csrf_token or not session_id:
            # ç›´æ¥è·å– HTML ä¸éœ€è¦ç™»å½•
            print("\né€‰æ‹©è¦è·å–çš„è®¨è®ºé¡µé¢:")
            print("a. è·å–æ‰€æœ‰è®¨è®ºé¡µé¢")
            print("æˆ–è¾“å…¥åˆ†ç±»ç¼–å· (1-12)")
            
            display_available_categories()
            
            fetch_input = input("\nè¯·é€‰æ‹©: ").strip().lower()
            
            if fetch_input == 'a':
                fetch_all_discussions()
            else:
                try:
                    cat_index = int(fetch_input) - 1
                    if 0 <= cat_index < len(PROBLEM_CATEGORIES):
                        discuss_id, filename, title = PROBLEM_CATEGORIES[cat_index]
                        fetch_and_save_discussion_html(discuss_id, filename)
                    else:
                        print("æ— æ•ˆçš„åˆ†ç±»ç¼–å·")
                except ValueError:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„é€‰é¡¹")
        else:
            client = LeetCodeClient(csrf_token, session_id)
            interactive_mode(client)


if __name__ == "__main__":
    main()
